{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook da ponderada de programação do curso de Sistemas de Informação - Módulo 6\n",
    "\n",
    "- Tema do módulo: Aplicação de Processamento de Linguagem Natural\n",
    "\n",
    "Este Colab tem como objetivo a resolução do exercício proposto sobre técnica de processamento de texto.\n",
    "\n",
    "Enunciado: Selecione 3 técnicas de pré processamento de textos e 10 frases da base de dados do projeto (combine com seu grupo para não usarem as mesmas frases). Organize o código python no Google Colab mostrando o resultado de cada técnica de pré-processamento. Adicione uma explicação para cada técnica de pré-processamento. Monte um pipeline de pré-processamento e apresente o resultado final. Ao menos uma técnica de pré-processamento deve ser implementada \"from scratch\" (sem o uso de biblioteca). Envie o link do Google Colab na Adalove. O link deve ter acesso liberado para o professor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>Emmanuel #Macron secretly aided #Uber lobbying...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Why would you park at YVR when you get long te...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Uber broke laws, duped police and secretly lob...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Uber Files: Massive leak reveals how top polit...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>This Uber I?m in is so quiet like put some mus...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29</td>\n",
       "      <td>I?ll cancel the Uber now</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>Since we all know Uber so well first hand, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>Yeah but where were you and the rest of the l...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>istg if this phone glitches or sum one more ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Yeah, I think we need to distinguish betwee...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment  sentiment\n",
       "0  23  Emmanuel #Macron secretly aided #Uber lobbying...         -1\n",
       "1  24  Why would you park at YVR when you get long te...         -1\n",
       "2  25  Uber broke laws, duped police and secretly lob...         -1\n",
       "3  27  Uber Files: Massive leak reveals how top polit...         -1\n",
       "4  28  This Uber I?m in is so quiet like put some mus...         -1\n",
       "5  29                           I?ll cancel the Uber now         -1\n",
       "6  30     Since we all know Uber so well first hand, ...         -1\n",
       "7  31   Yeah but where were you and the rest of the l...         -1\n",
       "8  32  istg if this phone glitches or sum one more ti...          0\n",
       "9  33     Yeah, I think we need to distinguish betwee...         -1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização sem uso de biblioteca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização de texto é uma prática comum dentro do processamento de lunguagem natural, pois ela permite que o texto seja dividido em partes menores, facilitando a análise de cada parte individualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emmanuel', '#Macron', 'secretly', 'aided', '#Uber', 'lobbying', 'drive', 'in', 'France,', 'leak', 'reveals', 'https://t.co/NgmK1A3bgV']\n"
     ]
    }
   ],
   "source": [
    "tokenizacao = df['comment'][0].split()\n",
    "print(tokenizacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_texto = ''\n",
    "def tokenizacao(texto):\n",
    "    \"\"\"\n",
    "    Realiza a tokenização de um texto, dividindo-o em palavras individuais.\n",
    "\n",
    "    Parâmetros:\n",
    "    texto (str): O texto a ser tokenizado.\n",
    "\n",
    "    Retorna:\n",
    "    list: Uma lista de palavras do texto. Retorna uma lista vazia se o texto for vazio.\n",
    "    \"\"\"\n",
    "     \n",
    "    string_texto = str(texto)\n",
    "    if len(string_texto) > 0:\n",
    "        return string_texto.split()\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emmanuel',\n",
       " '#Macron',\n",
       " 'secretly',\n",
       " 'aided',\n",
       " '#Uber',\n",
       " 'lobbying',\n",
       " 'drive',\n",
       " 'in',\n",
       " 'France,',\n",
       " 'leak',\n",
       " 'reveals',\n",
       " 'https://t.co/NgmK1A3bgV']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizacao(df['comment'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(string_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenização usando biblioteca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emmanuel', '#Macron', 'secretly', 'aided', '#Uber', 'lobbying', 'drive', 'in', 'France,', 'leak', 'reveals', 'https://t.co/NgmK1A3bgV']\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "frase = df['comment'][0]\n",
    "token_espaco = tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_espaco.tokenize(frase)\n",
    "print(token_frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Emmanuel', '#', 'Macron', 'secretly', 'aided', '#', 'Uber', 'lobbying', 'drive', 'in', 'France', ',', 'leak', 'reveals', 'https', '://', 't', '.', 'co', '/', 'NgmK1A3bgV']\n"
     ]
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "frase = df['comment'][0]\n",
    "token_pontuacao = tokenize.WordPunctTokenizer()\n",
    "token_frase = token_pontuacao.tokenize(frase)\n",
    "\n",
    "print(token_frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalização de texto usando biblioteca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Obtaining dependency information for unidecode from https://files.pythonhosted.org/packages/84/b7/6ec57841fb67c98f52fc8e4a2d96df60059637cba077edc569a302a8ffc7/Unidecode-1.3.8-py3-none-any.whl.metadata\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/235.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/235.5 kB 81.9 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 30.7/235.5 kB 81.9 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 30.7/235.5 kB 81.9 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 30.7/235.5 kB 81.9 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 41.0/235.5 kB 70.3 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 41.0/235.5 kB 70.3 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 61.4/235.5 kB 96.4 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 61.4/235.5 kB 96.4 kB/s eta 0:00:02\n",
      "   ------------- ------------------------- 81.9/235.5 kB 117.7 kB/s eta 0:00:02\n",
      "   ------------- ------------------------- 81.9/235.5 kB 117.7 kB/s eta 0:00:02\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 112.6/235.5 kB 142.6 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 122.9/235.5 kB 112.6 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 143.4/235.5 kB 127.1 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 194.6/235.5 kB 163.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 204.8/235.5 kB 170.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 225.3/235.5 kB 111.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 235.5/235.5 kB 115.3 kB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras = ' '.join([texto for texto in df['comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Emmanuel #Macron secretly aided #Uber lobbying drive in France, leak reveals  https://t.co/NgmK1A3bgV Why would you park at YVR when you get long term at Abbotsford airport, and Uber to that border crossing? A taxi to. SeaTac? Sounds like bullshit, there are frequent bus service from Blaine to SeaTac. 4 people in a cab w/luggage to SeaTac. What rubbish Uber broke laws, duped police and secretly lobbied governments, leak reveals  https://t.co/TD1ceDEuHr Uber Files: Massive leak reveals how top politicians secretly helped Uber - BBC News  Thousands of leaked files have exposed how Uber courted top politicians, and how far it went to avoid justice.  https://t.co/L5gjPXWrtp This Uber I?m in is so quiet like put some music on or somethingggggg I?ll cancel the Uber now    Since we all know Uber so well first hand, I feel it's safe to say that Monte and his office likely received Uber's documents (maybe a minor change or two) &amp; pushed the papers through.  I always thought Monte was better than that. ??  Yeah but where were you and the rest of the labor orgs when the cabbies were screaming about uber's corruption?  With a couple of exceptions, you all didn't care when we were the victims.  https://t.co/Oz7X090ToF istg if this phone glitches or sum one more time im gonna call an uber  https://t.co/xOgDoniKpr    Yeah, I think we need to distinguish between whether it's good for society to massively subsidize something taxi-like (it isn't) with whether Uber and Lyft are improvements over existing taxi systems (for the most part they are).\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Emmanuel #Macron secretly aided #Uber lobbying drive in France, leak reveals  https://t.co/NgmK1A3bgV Why would you park at YVR when you get long term at Abbotsford airport, and Uber to that border crossing? A taxi to. SeaTac? Sounds like bullshit, there are frequent bus service from Blaine to SeaTac. 4 people in a cab w/luggage to SeaTac. What rubbish Uber broke laws, duped police and secretly lobbied governments, leak reveals  https://t.co/TD1ceDEuHr Uber Files: Massive leak reveals how top politicians secretly helped Uber - BBC News  Thousands of leaked files have exposed how Uber courted top politicians, and how far it went to avoid justice.  https://t.co/L5gjPXWrtp This Uber I?m in is so quiet like put some music on or somethingggggg I?ll cancel the Uber now    Since we all know Uber so well first hand, I feel it's safe to say that Monte and his office likely received Uber's documents (maybe a minor change or two) &amp; pushed the papers through.  I always thought Monte was better than that. ??  Yeah but where were you and the rest of the labor orgs when the cabbies were screaming about uber's corruption?  With a couple of exceptions, you all didn't care when we were the victims.  https://t.co/Oz7X090ToF istg if this phone glitches or sum one more time im gonna call an uber  https://t.co/xOgDoniKpr    Yeah, I think we need to distinguish between whether it's good for society to massively subsidize something taxi-like (it isn't) with whether Uber and Lyft are improvements over existing taxi systems (for the most part they are).\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "acentos = [unidecode.unidecode(todas_palavras)]\n",
    "acentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_acentos = [unidecode.unidecode(texto) for texto in df['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emmanuel #Macron secretly aided #Uber lobbying drive in France, leak reveals  https://t.co/NgmK1A3bgV'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_acentos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>Emmanuel #Macron secretly aided #Uber lobbying...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Why would you park at YVR when you get long te...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>Uber broke laws, duped police and secretly lob...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>Uber Files: Massive leak reveals how top polit...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>This Uber I?m in is so quiet like put some mus...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29</td>\n",
       "      <td>I?ll cancel the Uber now</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>Since we all know Uber so well first hand, ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>Yeah but where were you and the rest of the l...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>istg if this phone glitches or sum one more ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Yeah, I think we need to distinguish betwee...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment  sentiment\n",
       "0  23  Emmanuel #Macron secretly aided #Uber lobbying...         -1\n",
       "1  24  Why would you park at YVR when you get long te...         -1\n",
       "2  25  Uber broke laws, duped police and secretly lob...         -1\n",
       "3  27  Uber Files: Massive leak reveals how top polit...         -1\n",
       "4  28  This Uber I?m in is so quiet like put some mus...         -1\n",
       "5  29                           I?ll cancel the Uber now         -1\n",
       "6  30     Since we all know Uber so well first hand, ...         -1\n",
       "7  31   Yeah but where were you and the rest of the l...         -1\n",
       "8  32  istg if this phone glitches or sum one more ti...          0\n",
       "9  33     Yeah, I think we need to distinguish betwee...         -1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\inteli\\anaconda3\\envs\\math-datascientist\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\inteli\\anaconda3\\envs\\math-datascientist\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\inteli\\anaconda3\\envs\\math-datascientist\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\inteli\\anaconda3\\envs\\math-datascientist\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\inteli\\anaconda3\\envs\\math-datascientist\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\inteli\\anaconda3\\envs\\math-datascientist\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emmanuel macron secretli aid uber lobbi drive franc leak reveal httpstcongmk1a3bgv would park yvr get long term abbotsford airport uber border cross taxi seatac sound like bullshit frequent bu servic blain seatac 4 peopl cab wluggag seatac rubbish uber broke law dupe polic secretli lobbi govern leak reveal httpstcotd1cedeuhr uber file massiv leak reveal top politician secretli help uber bbc news thousand leak file expos uber court top politician far went avoid justic httpstcol5gjpxwrtp uber im quiet like put music somethingggggg ill cancel uber sinc know uber well first hand feel safe say mont offic like receiv uber document mayb minor chang two amp push paper alway thought mont better yeah rest labor org cabbi scream uber corrupt coupl except didnt care victim httpstcooz7x090tof istg phone glitch sum one time im gon na call uber httpstcoxogdonikpr yeah think need distinguish whether good societi massiv subsid someth taxilik isnt whether uber lyft improv exist taxi system part\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Realiza a normalização de um texto, incluindo a conversão para minúsculas,\n",
    "    remoção de pontuação, tokenização, remoção de stopwords e stemização.\n",
    "\n",
    "    Parâmetros:\n",
    "    text (str): O texto a ser normalizado.\n",
    "\n",
    "    Retorna:\n",
    "    str: O texto normalizado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converte o texto para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove a pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokeniza o texto\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove as stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Faz a stemização\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Junta as palavras de volta em uma string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Exemplo de uso\n",
    "text = todas_palavras\n",
    "normalized_text = normalize_text(text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Emmanuel #Macron secretly aided #Uber lobbying drive in France, leak reveals  https://t.co/NgmK1A3bgV Why would you park at YVR when you get long term at Abbotsford airport, and Uber to that border crossing? A taxi to. SeaTac? Sounds like bullshit, there are frequent bus service from Blaine to SeaTac. 4 people in a cab w/luggage to SeaTac. What rubbish Uber broke laws, duped police and secretly lobbied governments, leak reveals  https://t.co/TD1ceDEuHr Uber Files: Massive leak reveals how top politicians secretly helped Uber - BBC News  Thousands of leaked files have exposed how Uber courted top politicians, and how far it went to avoid justice.  https://t.co/L5gjPXWrtp This Uber I?m in is so quiet like put some music on or somethingggggg I?ll cancel the Uber now    Since we all know Uber so well first hand, I feel it's safe to say that Monte and his office likely received Uber's documents (maybe a minor change or two) &amp; pushed the papers through.  I always thought Monte was better than that. ??  Yeah but where were you and the rest of the labor orgs when the cabbies were screaming about uber's corruption?  With a couple of exceptions, you all didn't care when we were the victims.  https://t.co/Oz7X090ToF istg if this phone glitches or sum one more time im gonna call an uber  https://t.co/xOgDoniKpr    Yeah, I think we need to distinguish between whether it's good for society to massively subsidize something taxi-like (it isn't) with whether Uber and Lyft are improvements over existing taxi systems (for the most part they are).\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todas_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Inteli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'https']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('https')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://t.co/Oz7X090ToF']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_urls(string):\n",
    "    \"\"\"\n",
    "    Encontra todas as URLs em uma string.\n",
    "\n",
    "    Parâmetros:\n",
    "    string (str): A string onde procurar as URLs.\n",
    "\n",
    "    Retorna:\n",
    "    list: Uma lista contendo todas as URLs encontradas na string.\n",
    "    \"\"\"\n",
    "\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    urls = re.findall(url_pattern, string)\n",
    "    return urls\n",
    "\n",
    "print(find_urls(df['comment'][7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://t.co/NgmK1A3bgV', 'https://t.co/NgmK1A3bgV']\n",
      "[]\n",
      "['https://t.co/TD1ceDEuHr', 'https://t.co/TD1ceDEuHr']\n",
      "['https://t.co/L5gjPXWrtp', 'https://t.co/L5gjPXWrtp']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['https://t.co/Oz7X090ToF', 'https://t.co/Oz7X090ToF']\n",
      "['https://t.co/xOgDoniKpr', 'https://t.co/xOgDoniKpr']\n",
      "[]\n",
      "['Emmanuel #Macron secretly aided #Uber lobbying drive France, leak reveals Emmanuel #Macron secretly aided #Uber lobbying drive France, leak reveals', 'would park YVR get long term Abbotsford airport, Uber border crossing? taxi to. SeaTac? Sounds like bullshit, frequent bus service Blaine SeaTac. 4 people cab w/luggage SeaTac. rubbish Why would park YVR get long term Abbotsford airport, Uber border crossing? A taxi to. SeaTac? Sounds like bullshit, frequent bus service Blaine SeaTac. 4 people cab w/luggage SeaTac. What rubbish', 'Uber broke laws, duped police secretly lobbied governments, leak reveals Uber broke laws, duped police secretly lobbied governments, leak reveals', 'Uber Files: Massive leak reveals top politicians secretly helped Uber - BBC News Thousands leaked files exposed Uber courted top politicians, far went avoid justice. Uber Files: Massive leak reveals top politicians secretly helped Uber - BBC News Thousands leaked files exposed Uber courted top politicians, far went avoid justice.', 'Uber I?m quiet like put music somethingggggg This Uber I?m quiet like put music somethingggggg', 'I?ll cancel Uber I?ll cancel Uber', \"Since know Uber well first hand, feel safe say Monte office likely received Uber's documents (maybe minor change two) &amp; pushed papers through. always thought Monte better that. ?? Since know Uber well first hand, I feel safe say Monte office likely received Uber's documents (maybe minor change two) &amp; pushed papers through. I always thought Monte better that. ??\", \"Yeah rest labor orgs cabbies screaming uber's corruption? couple exceptions, care victims. Yeah rest labor orgs cabbies screaming uber's corruption? With couple exceptions, care victims.\", 'istg phone glitches sum one time im gonna call uber istg phone glitches sum one time im gonna call uber', \"Yeah, think need distinguish whether good society massively subsidize something taxi-like (it isn't) whether Uber Lyft improvements existing taxi systems (for part are). Yeah, I think need distinguish whether good society massively subsidize something taxi-like (it isn't) whether Uber Lyft improvements existing taxi systems (for part are).\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Emmanuel #Macron secretly aided #Uber lobbying drive in France, leak reveals  https://t.co/NgmK1A3bgV'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_without_stopwords = list()\n",
    "\n",
    "for comment in df['comment']:\n",
    "    token_espaco = nltk.tokenize.WhitespaceTokenizer()\n",
    "    word_text = token_espaco.tokenize(comment)\n",
    "    new_comment = [w for w in word_text if not w.lower() in stopwords]\n",
    "    \n",
    "\n",
    "    for word in word_text:\n",
    "        if word not in stopwords:\n",
    "            new_comment.append(word)\n",
    "    \n",
    "    url_find = find_urls(' '.join(new_comment))\n",
    "\n",
    "    print(url_find)\n",
    "\n",
    "    new_comment = [word for word in new_comment if word not in url_find]\n",
    "\n",
    "    text_without_stopwords.append(' '.join(new_comment))\n",
    "print(text_without_stopwords)\n",
    "df['comment'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emmanuel macron secretli aid uber lobbi drive franc leak reveal httpstcongmk1a3bgv would park yvr get long term abbotsford airport uber border cross taxi seatac sound like bullshit frequent bu servic blain seatac 4 peopl cab wluggag seatac rubbish uber broke law dupe polic secretli lobbi govern leak reveal httpstcotd1cedeuhr uber file massiv leak reveal top politician secretli help uber bbc news thousand leak file expos uber court top politician far went avoid justic httpstcol5gjpxwrtp uber im quiet like put music somethingggggg ill cancel uber sinc know uber well first hand feel safe say mont offic like receiv uber document mayb minor chang two amp push paper alway thought mont better yeah rest labor org cabbi scream uber corrupt coupl except didnt care victim httpstcooz7x090tof istg phone glitch sum one time im gon na call uber httpstcoxogdonikpr yeah think need distinguish whether good societi massiv subsid someth taxilik isnt whether uber lyft improv exist taxi system part'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "def pipeline_pre_processamento(text):\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento de um texto, incluindo a conversão para minúsculas,\n",
    "    remoção de pontuação, tokenização, remoção de stopwords e stemização.\n",
    "\n",
    "    Parâmetros:\n",
    "    text (str): O texto a ser pré-processado.\n",
    "\n",
    "    Retorna:\n",
    "    str: O texto pré-processado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converte o texto para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove a pontuação\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokeniza o texto\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove as stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Faz a stemização\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Junta as palavras de volta em uma string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "pipeline_pre_processamento(todas_palavras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-datascientist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
